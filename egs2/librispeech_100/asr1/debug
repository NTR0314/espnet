/project/OML/master_theses/ozink/Waseda/espnet/tools/miniconda/envs/espnet/bin/python3 /project/OML/master_theses/ozink/Waseda/espnet/espnet2/bin/asr_inference.py --batch_size 1 --ngpu 1 --data_path_and_name_and_type dump/raw/test_clean/wav.scp,speech,kaldi_ark --data_path_and_name_and_type dump/raw/test_clean/text,text_gt,text --key_file exp/asr_train_asr_conformer_lr2e-3_warmup15k_amp_nondeterministic_raw_en_bpe5000_sp/decode_asr_asr_model_valid.acc.ave/test_clean/logdir/keys.1.scp --asr_train_config exp/asr_train_asr_conformer_lr2e-3_warmup15k_amp_nondeterministic_raw_en_bpe5000_sp/config.yaml --asr_model_file exp/asr_train_asr_conformer_lr2e-3_warmup15k_amp_nondeterministic_raw_en_bpe5000_sp/valid.acc.ave.pth --output_dir exp/asr_train_asr_conformer_lr2e-3_warmup15k_amp_nondeterministic_raw_en_bpe5000_sp/decode_asr_asr_model_valid.acc.ave/test_clean/logdir/output.1 --config conf/decode_asr.yaml
2024-04-05 08:36:17,492 (abs_task:2088) INFO: config file: exp/asr_train_asr_conformer_lr2e-3_warmup15k_amp_nondeterministic_raw_en_bpe5000_sp/config.yaml
2024-04-05 08:36:17,857 (asr:538) INFO: Vocabulary size: 5000
2024-04-05 08:36:20,498 (asr_inference:357) INFO: BatchBeamSearch implementation is selected.
2024-04-05 08:36:20,502 (asr_inference:368) INFO: Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(5000, 256)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=256, out_features=5000, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=256, out_features=256, bias=True)
            (linear_k): Linear(in_features=256, out_features=256, bias=True)
            (linear_v): Linear(in_features=256, out_features=256, bias=True)
            (linear_out): Linear(in_features=256, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=256, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=256, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2024-04-05 08:36:20,502 (asr_inference:369) INFO: Decoding device=cuda, dtype=float32
2024-04-05 08:36:20,587 (asr_inference:449) INFO: Text tokenizer: SentencepiecesTokenizer(model="data/en_token_list/bpe_unigram5000/bpe.model")
2024-04-05 08:36:20,610 (asr:427) INFO: ['text_gt']
2024-04-05 08:36:20,746 (asr:509) INFO: Optional Data Names: ('text_gt',)
2024-04-05 08:36:21,205 (asr_inference:832) INFO: batch={'speech': tensor([0.0003, 0.0003, 0.0004,  ..., 0.0021, 0.0021, 0.0016]), 'text_gt': tensor([  11, 2115,   54,   59,   29,  274,   31,  190,   21, 1091,  555,  712,
           3,    4,  460,  206,   22,    3,    4, 4811,   30, 1601,  281, 2735,
           4, 1606, 1132,   22,  439, 2603,    6,   29, 1186,  431,   67,    8,
         889, 4716,    9, 2749, 1606,  964,    9,  584,  109,  250])}
Traceback (most recent call last):
  File "/project/OML/master_theses/ozink/Waseda/espnet/tools/miniconda/envs/espnet/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/project/OML/master_theses/ozink/Waseda/espnet/tools/miniconda/envs/espnet/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/project/OML/master_theses/ozink/Waseda/espnet/espnet2/bin/asr_inference.py", line 1138, in <module>
    main()
  File "/project/OML/master_theses/ozink/Waseda/espnet/espnet2/bin/asr_inference.py", line 1134, in main
    inference(**kwargs)
  File "/project/OML/master_theses/ozink/Waseda/espnet/espnet2/bin/asr_inference.py", line 836, in inference
    results = speech2text(**batch)
  File "/project/OML/master_theses/ozink/Waseda/espnet/tools/miniconda/envs/espnet/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
TypeError: __call__() got an unexpected keyword argument 'text_gt'
